{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrds\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 : Import the articles for each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the directory path\n",
    "directory = \"../articles\"\n",
    "\n",
    "# initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# loop through each directory in the Articles directory\n",
    "for subdir in os.listdir(directory):\n",
    "    subdir_path = os.path.join(directory, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        # loop through each file in the directory\n",
    "        for filename in os.listdir(subdir_path):\n",
    "            file_path = os.path.join(subdir_path, filename)\n",
    "            if os.path.isfile(file_path) and filename.endswith(\".json\"):\n",
    "                # read the JSON file\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    json_obj = json.load(f)\n",
    "                # extract the relevant information from the JSON object\n",
    "                _id = json_obj[\"_id\"]\n",
    "                #name of the directory is the company name\n",
    "                company = subdir\n",
    "                title = json_obj[\"title\"]\n",
    "                text = json_obj[\"text\"]\n",
    "                cleaned_text = re.sub(r'<p>|</p>', '', text)\n",
    "                #Change to timestamp YYYY-MM-DD HH:MM:SS\n",
    "                published = json_obj[\"published\"]\n",
    "                timestamp = datetime.strptime(published, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "                data.append([_id, company, title, cleaned_text, timestamp])\n",
    "        \n",
    "df = pd.DataFrame(data, columns=[\"id\", \"company\", \"title\", \"text\", \"published\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates texts\n",
    "df = df.sort_values(['company', 'published'])\n",
    "df = df.drop_duplicates(subset=['company','text'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print company names\n",
    "company_to_ticker = {\n",
    "    '3M_Company': 'MMM',\n",
    "    'American_Express_co': 'AXP',\n",
    "    'Amgen_Inc': 'AMGN',\n",
    "    'Apple_Inc': 'AAPL',\n",
    "    'Boeing_Co': 'BA',\n",
    "    'Caterpillar_Inc': 'CAT',\n",
    "    'Chevron_Corporation': 'CVX',\n",
    "    'Cisco_Systems_Inc': 'CSCO',\n",
    "    'Coca_Cola_Co': 'KO',\n",
    "    'Dow_Inc': 'DOW',\n",
    "    'Goldman_Sachs_Group_Inc': 'GS',\n",
    "    'Home_Depot_Inc': 'HD',\n",
    "    'Honeywell_International_Inc': 'HON',\n",
    "    'Intel_Corporation': 'INTC',\n",
    "    'International_Business_Machines_Corporation': 'IBM',\n",
    "    'JPMorgan_Chase_Co': 'JPM',\n",
    "    'Johnson_Johnson': 'JNJ',\n",
    "    'McDonald_s_Corporation': 'MCD',\n",
    "    'Merck_Co_Inc': 'MRK',\n",
    "    'Microsoft_Corporation': 'MSFT',\n",
    "    'Nike_Inc': 'NKE',\n",
    "    'Procter_Gamble_Co': 'PG',\n",
    "    'Salesforce_Inc': 'CRM',\n",
    "    'The_Walt_Disney_Company': 'DIS',\n",
    "    'Travelers_Companies_Inc': 'TRV',\n",
    "    'Unitedhealth_Group_Incorporated': 'UNH',\n",
    "    'Verizon_communications_Inc': 'VZ',\n",
    "    'Visa_Inc': 'V',\n",
    "    'Walgreens_Boots_Alliance_Inc': 'WBA',\n",
    "    'Walmart_Inc': 'WMT'\n",
    "}\n",
    "\n",
    "#Replace company names with ticker symbols\n",
    "df['ticker'] = df['company'].map(company_to_ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 : Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 : Clean articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpers functions for cleaning text\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def remove_ftcom_and_after(text):\n",
    "    index = text.find(\"FT.com\")\n",
    "    return text[:index] if index != -1 else text\n",
    "\n",
    "def remove_source_and_after(text) : \n",
    "    index  = text.find(\"Source:\")\n",
    "    return text[:index] if index != -1 else text\n",
    "\n",
    "def replace_PG(text):\n",
    "    return text.replace(\"P&amp;G\", \"Procter & Gamble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove html tags\n",
    "df['text'] = df['text'].apply(remove_html_tags)\n",
    "\n",
    "#Remove FT.com and after\n",
    "df['text'] = df['text'].apply(remove_ftcom_and_after)\n",
    "\n",
    "#Remove Source: and after\n",
    "df['text'] = df['text'].apply(remove_source_and_after)\n",
    "\n",
    "#Change P&G tokens \n",
    "df['text'] = df['text'].apply(replace_PG)\n",
    "\n",
    "#Remove &#xa0; and replace with space\n",
    "df['text'] = df['text'].str.replace('&#xa0;', ' ')\n",
    "\n",
    "#Remove \"Follow @FT\" and after\n",
    "df['text'] = df['text'].apply(lambda x: x.split(\"Follow @FT\")[0])\n",
    "\n",
    "#Transform to lower case all text\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "#Remove rows with \"Please sign up here\n",
    "df = df[df['text'] != \"Please sign up here\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from difflib import SequenceMatcher\n",
    "\n",
    "# def similar_words(text, target, similarity_threshold=0.8):\n",
    "#     # Split the text into words\n",
    "#     words = text.split()\n",
    "\n",
    "#     # Find words that are similar to the target\n",
    "#     similar = [word for word in words if SequenceMatcher(None, word, target).ratio() > similarity_threshold]\n",
    "\n",
    "#     return similar\n",
    "\n",
    "# df['similar_words'] = df.apply(lambda row: similar_words(row['text'], row['company']), axis=1)\n",
    "\n",
    "#They are going to give us the list of words that are similar to the company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_to_company = {'MMM' : '3m',\n",
    "                     'AXP' : 'american express',\n",
    "                     'AMGN' : 'amgen',\n",
    "                     'AAPL' : 'apple',\n",
    "                     'BA' : 'boeing',\n",
    "                     'CAT' : 'caterpillar',\n",
    "                     'CVX' : 'chevron',\n",
    "                     'CSCO' : 'cisco',\n",
    "                     'KO' : 'coca cola',\n",
    "                     'DOW' : 'dow',\n",
    "                     'GS' : 'goldman sachs',\n",
    "                     'HD' : 'home depot',\n",
    "                     'HON' : 'honeywell',\n",
    "                     'INTC' : 'intel',\n",
    "                     'IBM' : 'ibm',\n",
    "                     'JPM' : 'jpmorgan',\n",
    "                     'JNJ' : 'johnson & johnson',\n",
    "                     'MCD' : 'mcdonalds',\n",
    "                     'MRK' : 'merck',\n",
    "                     'MSFT' : 'microsoft',\n",
    "                     'NKE' : 'nike',\n",
    "                     'PG' : 'procter & gamble',\n",
    "                     'CRM' : 'salesforce',\n",
    "                     'DIS' : 'walt disney',\n",
    "                     'TRV' : 'travelers',\n",
    "                     'UNH' : 'united health',\n",
    "                     'VZ' : 'verizon',\n",
    "                     'V' : 'visa',\n",
    "                     'WBA' : 'walgreens',\n",
    "                     'WMT' : 'walmart'}\n",
    "\n",
    "df['company_words'] = df['ticker'].map(ticker_to_company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hv/67_537bx1cb2f2v1x8cw8k0m0000gn/T/ipykernel_10431/788995560.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace('[^\\w\\s\\.]','')\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].str.lower()\n",
    "#Delete punctuation except '.'\n",
    "df['text'] = df['text'].str.replace('[^\\w\\s\\.]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sentences(text, company_words, before=0, after=0):\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('. ')\n",
    "\n",
    "    # Find the indices of the sentences where the company name appears\n",
    "    indices = [i for i, sentence in enumerate(sentences) if company_words in sentence]\n",
    "\n",
    "    # If the company name does not appear, return an empty list\n",
    "    if not indices:\n",
    "        return []\n",
    "\n",
    "    # Initialize the list of selected sentences\n",
    "    selected = []\n",
    "\n",
    "    # Iterate over the indices\n",
    "    for i in range(len(indices)):\n",
    "        # If this is the first index, select the sentences before it\n",
    "        if i == 0:\n",
    "            start = max(0, indices[i]-before)\n",
    "        # Otherwise, select the sentences after the previous index\n",
    "        else:\n",
    "            start = max(indices[i-1] + after + 1, indices[i]-before)\n",
    "\n",
    "        # Select the sentences before, at, and after the current index\n",
    "        end = indices[i] + after + 1\n",
    "        selected.extend(sentences[start:end])\n",
    "\n",
    "    return selected\n",
    "\n",
    "# Apply the function to each row in the dataframe : we only keep the sentences that contain the company name and the sentences before and after\n",
    "df['sentences'] = df.apply(lambda row: select_sentences(row['text'], row['company_words']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For delete the sentences that contain Listen on:\n",
    "df['sentences'] = df.apply(lambda row: [sentence for sentence in row['sentences'] if 'listen on:' not in sentence], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the text column\n",
    "df = df.drop(columns=['text'])\n",
    "\n",
    "#Copy the dataframe\n",
    "df_articles_cleaned = df.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we remove the articles that don't contain sentences about the company we are focusing on, so articles with an empty list of sentences\n",
    "df_articles_cleaned = df_articles_cleaned[df_articles_cleaned.astype(str)['sentences'] != '[]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 : Dive into the articles and their structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will dive into the distribution of the number of articles per company\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "count = df_articles_cleaned.groupby('company').count()['id'].sort_values(ascending = False)\n",
    "\n",
    "ax = sns.barplot(\n",
    "    x = count.index , \n",
    "    y = count.values , \n",
    "    color = 'teal', \n",
    ")\n",
    "\n",
    "ax.set_title('Distribution of the number of articles per company')\n",
    "ax.set_xlabel('Company')\n",
    "ax.set_ylabel('Number of articles')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker\n",
       "AAPL    18755\n",
       "GS       9427\n",
       "MSFT     9318\n",
       "JPM      8409\n",
       "WMT      6386\n",
       "BA       3831\n",
       "INTC     3786\n",
       "DIS      3201\n",
       "NKE      2326\n",
       "MCD      2233\n",
       "CVX      2221\n",
       "MRK      1817\n",
       "V        1792\n",
       "CRM      1675\n",
       "HD       1474\n",
       "VZ       1474\n",
       "IBM      1275\n",
       "CSCO     1050\n",
       "WBA       989\n",
       "AXP       969\n",
       "CAT       706\n",
       "MMM       646\n",
       "HON       496\n",
       "AMGN      357\n",
       "DOW       106\n",
       "TRV        55\n",
       "UNH        11\n",
       "KO          9\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles_cleaned.groupby('ticker').count()['id'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a really important representation of Apple articles and it dominates all the other companies in the dataset. We will further try to understand the articles related to Apple in order to check wether there is any discontinuities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the distribution of the number of sentences per article\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "count = df_articles_cleaned['sentences'].apply(lambda x: len(x))\n",
    "\n",
    "sns.histplot(count, color='teal', bins=50 , log = True)\n",
    "\n",
    "plt.title('Distribution of the number of sentences per article')\n",
    "plt.xlabel('Number of sentences')\n",
    "plt.ylabel('Number of articles')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plotted the log distribution of the number of sentences per article. We see that a really important number of articles have only a few sentences, i.e between 1 and 4. This is interesting as it shows that among the articles that we did not remove because they were not including texts about the company, contain only a small fraction of information about the specific company. So by only looking at the sentences talking about the company + previous and following sentence we can be quite sure that we kept the necessary information related to the company. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We know that most NLP models require in input a specific number of tokens so we will check the distribution of the number of tokens per sentence\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "count = df_articles_cleaned['sentences'].apply(lambda x: [len(sentence.split()) for sentence in x]).explode()\n",
    "\n",
    "sns.histplot(count, color='teal', bins=50 , log = True)\n",
    "\n",
    "plt.title('Distribution of the number of tokens per sentence')\n",
    "plt.xlabel('Number of tokens')\n",
    "plt.ylabel('Number of sentences')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "series = pd.Series(count.values).astype(float)  # Convert to float if they are all numbers\n",
    "series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have most of the sentences following the same pattern, i.e, they all have a small number of tokens, so they will be easily fed to future NLP models, we see that 75% of the sentences have less than 49 tokens. However, we still have to deal with the senteces that have more than 500 tokens as we won't be able to feed them to NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the articles that have sentences with more than 500 tokens\n",
    "df_articles_cleaned['len_sentences'] = df_articles_cleaned['sentences'].apply(lambda x: [len(sentence.split()) for sentence in x])\n",
    "\n",
    "#We print the number of articles that have sentences with more than 500 tokens\n",
    "articles_more_500_tokens = df_articles_cleaned[df_articles_cleaned[\"len_sentences\"].apply(lambda x: max(x)) > 500]\n",
    "print(f'The number of articles that have sentences with more than 500 tokens is {len(articles_more_500_tokens)}')\n",
    "\n",
    "#For each company containing articles with sentences with more than 500 tokens, we print he ratio of articles with sentences with more than 500 tokens\n",
    "for company in articles_more_500_tokens['company'].unique():\n",
    "    print(f'The ratio of articles with sentences with more than 500 tokens for {company} is {len(articles_more_500_tokens[articles_more_500_tokens[\"company\"] == company]) / len(df_articles_cleaned[df_articles_cleaned[\"company\"] == company])}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for each of the company that have more that articles with more than 500 tokens, the number of those article is unsignificant regarding the total number of articles we have for those companies so we decide to remove those articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the articles that have sentences with more than 500 tokens\n",
    "df_articles_cleaned = df_articles_cleaned[df_articles_cleaned[\"len_sentences\"].apply(lambda x: max(x)) <= 500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 : Dive into the balance of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We plot the number of articles we have over time for each company with a subplot for each company\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "for i, company in enumerate(df_articles_cleaned['company'].unique()):\n",
    "    plt.subplot(4, 8, i+1)\n",
    "    plt.title(company)\n",
    "    df_articles_cleaned[df_articles_cleaned['company'] == company].groupby('published').count()['id'].plot()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframe to a csv file\n",
    "df.to_csv('../data/news_cleaned.csv', index=True, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
