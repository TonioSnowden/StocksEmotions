{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the dataset\n",
    "data_path = '/Users/romainberquet/Desktop/epfl/ml-fin/Projet/archive/all-data.csv'\n",
    "df = pd.read_csv(data_path, \n",
    "                   encoding='unicode_escape',\n",
    "                   names=['Sentiment', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Text' : 'text' , 'Sentiment' : 'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"]=df[\"text\"].str.lower() #We convert our texts to lowercase.\n",
    "df[\"text\"]=df[\"text\"].str.replace(\"[^\\w\\s]\",\"\") #We remove punctuation marks from our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform data exploration on the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x= df.sentiment.unique(), y=df.sentiment.value_counts())\n",
    "ax.set(xlabel='Sentiment', ylabel='Number of articles' , title='Number of articles per type of sentiment')\n",
    "plt.show()\n",
    "\n",
    "sentiment_proportions = df['sentiment'].value_counts(normalize=True)\n",
    "print(f\"The proportion of each sentiment in the dataset is \\n {sentiment_proportions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is relatively imbalanced with a high proportion of neutral comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the distribution of the number of words per article\n",
    "df['text'].apply(lambda x: len(x.split(\" \"))).mean()\n",
    "df['text'].apply(lambda x: len(x.split(\" \"))).plot(kind='hist' , bins=  75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have no empty articles and the distribution of the length is relatively well balanced among articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We map the sentiment to a numerical value positive : 1, neutral 0 and negative 2\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'neutral': 0, 'negative': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark model : Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(df['text'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data into a bag-of-words model\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train_bow)\n",
    "X_test_bow = vectorizer.transform(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classifier (X_train, y_train, X_test):\n",
    "    # Train the model\n",
    "    svm = SVC(kernel='linear')\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the sentiment for the test data\n",
    "    y_pred = svm.predict(X_test)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_classifier (X_train, y_train, X_test):\n",
    "    # Train a Gradient Boosting classifier\n",
    "    xgb_clf = XGBClassifier()\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifier (X_train, y_train, X_test):\n",
    "    # Train a Naive Bayes classifier\n",
    "    nb_clf = MultinomialNB()\n",
    "    nb_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = nb_clf.predict(X_test)\n",
    "    # Evaluate using accuracy, precision, recall, F1-score as before\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_classifier(X_train, y_train, X_test):\n",
    "\n",
    "    if isinstance(X_train, csr_matrix):\n",
    "        X_train_dense = X_train.toarray()\n",
    "    else:\n",
    "        X_train_dense = X_train\n",
    "\n",
    "    if isinstance(X_test, csr_matrix):\n",
    "        X_test_dense = X_test.toarray()\n",
    "    else:\n",
    "        X_test_dense = X_test\n",
    "\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(512, 256), activation='relu', solver='adam', \n",
    "                        max_iter=35, batch_size=128, verbose=True)\n",
    "\n",
    "    mlp.fit(X_train_dense, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = mlp.predict(X_test_dense)\n",
    "\n",
    "    return y_pred\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_classifier(X_train, y_train, X_test):\n",
    "    # Train a logistic regression classifier\n",
    "    lr_clf = LogisticRegression(max_iter=1000)\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = lr_clf.predict(X_test)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification using bag of words\n",
    "y_pred_svm_bow = svm_classifier(X_train_bow, y_train_bow, X_test_bow)\n",
    "y_pred_xgb_bow  = gradient_boosting_classifier(X_train_bow, y_train_bow, X_test_bow)\n",
    "y_pred_nb_bow  = naive_bayes_classifier(X_train_bow, y_train_bow, X_test_bow)\n",
    "y_pred_mlp_bow  = mlp_classifier(X_train_bow, y_train_bow, X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_performance(df, model_name, y_test, y_pred):\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Extract weighted average metrics\n",
    "    weighted_avg = report['weighted avg']\n",
    "    precision = weighted_avg['precision']\n",
    "    recall = weighted_avg['recall']\n",
    "    f1_score = weighted_avg['f1-score']\n",
    "\n",
    "    # Create a new DataFrame for the row to be added\n",
    "    new_row_df = pd.DataFrame({'Model': [model_name], \n",
    "                               'Precision': [precision], \n",
    "                               'Recall': [recall], \n",
    "                               'F1-Score': [f1_score]})\n",
    "\n",
    "    # Concatenate the new row with the existing DataFrame\n",
    "    df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a dataframe that will contain the performance of the different models\n",
    "performance_df_bow = pd.DataFrame(columns=['Model', 'Precision', 'Recall', 'F1-Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(performance_df):    \n",
    "    # Plotting\n",
    "    n_models = len(performance_df_bow)\n",
    "    ind = np.arange(n_models)  # the x locations for the groups\n",
    "    width = 0.25  # the width of the bars\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"Set2\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plotting each metric\n",
    "    rects1 = ax.bar(ind - width, performance_df['Precision'], width, label='Precision')\n",
    "    rects2 = ax.bar(ind, performance_df['Recall'], width, label='Recall')\n",
    "    rects3 = ax.bar(ind + width, performance_df['F1-Score'], width, label='F1-Score')\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Performance by Model and Metric')\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(performance_df['Model'])\n",
    "    ax.legend()\n",
    "\n",
    "    # Attach a text label above each bar in *rects*, displaying its height.\n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(round(height, 2)),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "\n",
    "    # Call the function to attach the labels\n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "    autolabel(rects3)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df_bow = update_performance(performance_df_bow, 'BoW & SVM', y_test_bow, y_pred_svm_bow)\n",
    "performance_df_bow = update_performance(performance_df_bow, 'BoW & XGBoost', y_test_bow, y_pred_xgb_bow)\n",
    "performance_df_bow = update_performance(performance_df_bow, 'BoW & Naive Bayes', y_test_bow, y_pred_nb_bow)\n",
    "performance_df_bow = update_performance(performance_df_bow, 'BoW & MLP', y_test_bow, y_pred_mlp_bow)\n",
    "\n",
    "plot_performance(performance_df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark model : TF-IDF (Term Frequency-Inverse Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(df['text'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_tfidf)\n",
    "\n",
    "# Only transform the test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification using bag of words\n",
    "y_pred_svm_tfidf = svm_classifier(X_train_tfidf, y_train_tfidf, X_test_tfidf)\n",
    "y_pred_xgb_tfidf  = gradient_boosting_classifier(X_train_tfidf, y_train_tfidf, X_test_tfidf)\n",
    "y_pred_nb_tfidf  = naive_bayes_classifier(X_train_tfidf, y_train_tfidf, X_test_tfidf)\n",
    "y_pred_mlp_tfidf = mlp_classifier(X_train_tfidf, y_train_tfidf, X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df_tfidf = pd.DataFrame(columns=['Model', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "performance_df_tfidf = update_performance(performance_df_tfidf, 'TF-IDF & SVM', y_test_tfidf, y_pred_svm_tfidf)\n",
    "performance_df_tfidf = update_performance(performance_df_tfidf, 'TF-IDF & XGBoost', y_test_tfidf, y_pred_xgb_tfidf)\n",
    "performance_df_tfidf = update_performance(performance_df_tfidf, 'TF-IDF & Naive Bayes', y_test_tfidf, y_pred_nb_tfidf)\n",
    "performance_df_tfidf = update_performance(performance_df_tfidf, 'TF-IDF & MLP', y_test_tfidf, y_pred_mlp_tfidf)\n",
    "\n",
    "plot_performance(performance_df_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark model : GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_model(glove_file_path):\n",
    "    print(\"Loading Glove Model\")\n",
    "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "        glove_model = {}\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array([float(val) for val in split_line[1:]])\n",
    "            glove_model[word] = embedding\n",
    "        print(f\"{len(glove_model)} words loaded!\")\n",
    "        return glove_model\n",
    "\n",
    "glove_path = '/Users/romainberquet/Desktop/epfl/ml-fin/Projet/glove.6B/glove.6B.300d.txt'  \n",
    "glove_model = load_glove_model(glove_path)\n",
    "\n",
    "X_train_glove, X_test_glove, y_train_glove, y_test_glove = train_test_split(df['text'], df['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(glove_model, doc):\n",
    "    words = doc.split()\n",
    "    word_vectors = [glove_model[word] for word in words if word in glove_model]\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(300)  # 300 is the GloVe vector size\n",
    "\n",
    "X_train_vectors_glove = np.array([document_vector(glove_model, text) for text in X_train_glove])\n",
    "X_test_vectors_glove = np.array([document_vector(glove_model, text) for text in X_test_glove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification using bag of words\n",
    "y_pred_svm_Glove = svm_classifier(X_train_vectors_glove, y_train_glove, X_test_vectors_glove)\n",
    "y_pred_xgb_Glove  = gradient_boosting_classifier(X_train_vectors_glove, y_train_glove, X_test_vectors_glove)\n",
    "y_pred_mlp_Glove = mlp_classifier(X_train_vectors_glove, y_train_glove, X_test_vectors_glove)\n",
    "\n",
    "#Add a logistic regression classifier\n",
    "y_pred_lr_Glove = logistic_regression_classifier(X_train_vectors_glove, y_train_glove, X_test_vectors_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df_glove = pd.DataFrame(columns=['Model', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "performance_df_glove = update_performance(performance_df_glove, 'Glove & SVM', y_test_glove, y_pred_svm_Glove)\n",
    "performance_df_glove = update_performance(performance_df_glove, 'Glove & XGBoost', y_test_glove, y_pred_xgb_Glove)\n",
    "performance_df_glove = update_performance(performance_df_glove, 'Glove & MLP', y_test_glove, y_pred_mlp_Glove)\n",
    "performance_df_glove = update_performance(performance_df_glove, 'Glove & Logistic Regression', y_test_glove, y_pred_lr_Glove)\n",
    "\n",
    "\n",
    "plot_performance(performance_df_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Financial BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_finbert = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model_finbert = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_finbert = df['text'].to_list()\n",
    "y_test_finbert = df['sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_model(X_train , tokenizer , model) :\n",
    "    preds = []\n",
    "    preds_proba = []\n",
    "    tokenizer_kwargs = {\"padding\": True, \"truncation\": True, \"max_length\": 512}\n",
    "    for x in X_train:\n",
    "        with torch.no_grad():\n",
    "            input_sequence = tokenizer(x, return_tensors=\"pt\", **tokenizer_kwargs)\n",
    "            logits = model(**input_sequence).logits\n",
    "            scores = {\n",
    "            k: v\n",
    "            for k, v in zip(\n",
    "                model.config.id2label.values(),\n",
    "                scipy.special.softmax(logits.numpy().squeeze()),\n",
    "            )\n",
    "        }\n",
    "        sentiment = max(scores, key=scores.get)\n",
    "        probability = max(scores.values())\n",
    "        preds.append(sentiment)\n",
    "        preds_proba.append(probability)\n",
    "\n",
    "    return preds, preds_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map the predictions to numerical values\n",
    "y_pred_finbert, y_pred_proba_finbert = classify_model(X_test_finbert, tokenizer_finbert, model_finbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asses the performance of the model\n",
    "finbert_performance = pd.DataFrame(classification_report(y_pred_finbert, y_test_finbert, output_dict=True))\n",
    "finbert_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FinBert is the most accurate model, we will look at the predictions it made and the probability it assigned to each prediction\n",
    "finbert_df = pd.DataFrame({'text': X_test_finbert, 'sentiment': y_test_finbert, 'prediction': y_pred_finbert, 'probability': y_pred_proba_finbert})\n",
    "\n",
    "#We look at the articles that were misclassified\n",
    "missclassified = finbert_df[finbert_df['sentiment'] != finbert_df['prediction']]\n",
    "\n",
    "#Count the number of pairs of sentiment and prediction\n",
    "missclassified = missclassified.groupby(['sentiment', 'prediction']).count().reset_index().drop(columns=['probability'])\n",
    "\n",
    "#plot a heat map of the number of misclassified articles\n",
    "missclassified = missclassified.pivot(index='sentiment', columns='prediction', values='text')\n",
    "sns.heatmap(missclassified, annot=True, cmap=\"viridis\", linewidths=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the distribution of the probability of the prediction\n",
    "missclassified = finbert_df[finbert_df['sentiment'] != finbert_df['prediction']]\n",
    "\n",
    "# Improved version\n",
    "sns.displot(missclassified, x=\"probability\", hue=\"prediction\", kind=\"kde\", fill=True, height=6)\n",
    "plt.title(\"Probability Distribution by Prediction\", fontsize=16)  \n",
    "plt.xlabel(\"Probability\", fontsize=14)\n",
    "plt.ylabel(\"Density\", fontsize=14) \n",
    "plt.xticks(fontsize=12)  \n",
    "plt.yticks(fontsize=12) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_roberta = AutoTokenizer.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "model_roberta = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_roberta = df['text'].to_list()\n",
    "y_test_roberta = df['sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map the predictions to numerical values\n",
    "y_pred_roberta, y_pred_proba_roberta = classify_model(X_test_roberta , tokenizer_roberta , model_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asses the performance of the model\n",
    "roberta_performance = pd.DataFrame(classification_report(y_pred_roberta, y_test_roberta, output_dict=True))\n",
    "roberta_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
