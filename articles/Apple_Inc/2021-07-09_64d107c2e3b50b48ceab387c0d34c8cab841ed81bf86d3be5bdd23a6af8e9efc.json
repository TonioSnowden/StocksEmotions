{
  "_id": "64d107c2e3b50b48ceab387c0d34c8cab841ed81bf86d3be5bdd23a6af8e9efc",
  "feed": "wall-street-journal",
  "title": "The Future of Everything: The Artificial Intelligence Issue --- Making Senses",
  "text": "<p>Hype around AI is running high, and much of the research is in early stages. Here, we look at 10 working models and prototypes of AI with sensory abilities.</p><p>TRANSPARENCY</p><p>Robots aren't good at handling glass bottles or clear plastic cups. That is because most visual systems use infrared beams, known as depth sensors, to determine the shape of objects, and they shine right through transparent materials, capturing only vague shadows. Engineers at Carnegie Mellon University paired a depth sensor with a standard color camera to fill in data gaps by catching hues of red, green and blue around the edges of see-through objects. They then retrained the system to recognize these subtle visual cues and enable a robotic arm to adjust its grip. \"Your vision is more similar to the way the color camera works,\" says David Held, an assistant professor at Carnegie Mellon's Robotics Institute. \"You don't send out lasers and see how long they take to bounce back.\"</p><p>ROBOTS WITH IMAGINATION</p><p>Software maker OpenAI has developed an AI-powered application, dubbed DALL-E, that can generate images of fanciful scenes from text typed in by users. CLIP, a second app, goes the other way, generating descriptive text from inputted images. Both apps use a neural network that \"chews and digests\" data from a vast library of images and text, until it generates entirely new images or text that match the initial input, says Ilya Sutskever, OpenAI's co-founder and chief scientist. The systems could be used to generate visual versions of textbooks or photorealistic movies from a script, among other applications.</p><p>YOUR CUP OF TEA?</p><p>Scientists at International Business Machines Corp.'s research division in Zurich are developing a pocket-size device, dubbed Hypertaste -- also called an electric tongue -- that consists of electrochemical sensors made of an array of electrodes. Set on the edge of a glass, the device's electrodes respond to different molecules in liquids that give wine or soda a unique taste, creating a code of electrical signals that become the drink's \"digital fingerprint,\" says Patrick Ruch, the project's lead researcher. Machine-learning algorithms compare these fingerprints in a growing database. Once a match is found, the results can then be sent to a smartphone app enabling users to identify the drink as a Pinot Noir or a Pepsi. By fine-tuning the data, the process can also predict the intensity of drinks, like strong coffee.</p><p>A TASTY DATABASE</p><p>Gastrograph AI, created by New York-based Analytical Flavor Systems Inc., is a self-learning platform that aims to predict how people will react to new food products. It works by training a digital model to cross-reference data gathered from thousands of consumers who rate dishes on a mobile app with categories like fishy, gamy, woody, fruity or floral, and more nuanced subcategories, like a tinge of green apple or navel orange, says Jason Cohen, the startup's founder and CEO. The system breaks down the data into complex spider-graph flavor and preference patterns, kept in an ever expanding database, which can be used to simulate how consumers in a given market will take to novel products, such as litchi beer or watermelon popcorn.</p><p>ALL THE FEELS</p><p>Warehouse and production-line robots handle packages or car parts all the time. But do they feel them? GelSight, a technology developed by researchers at the Massachusetts Institute of Technology's Computer Science and Artificial Intelligence Lab, uses a robotic arm with a small rubber block on the end that can be pressed into objects, such as a dime or pencil. A camera converts the imprint into a 3-D digital image, enabling computer-vision and algorithmic models to determine the object's size and shape, and feed crucial information -- like how much force is needed to hold or balance it -- back to a robotic gripper. Once an object is digitized, \"anything you can do with an image, you can do with touch,\" says Ted Adelson, a neuroscientist at MIT who led the research. The technology, which in subsequent testing has been reduced to the size of a fingertip, gives robots greater dexterity, and could be applied everywhere from warehouses to operating rooms.</p><p>THE HUMAN TOUCH</p><p>Humans take a lot of complex tactile abilities for granted, like finding keys in a pocket or buttoning a shirt without looking. Robots have yet to figure that out, says Yunzhu Li, an MIT researcher. Mr. Li's team is working to bridge the gap between touch and sight by training an AI system to predict what a seen object feels like and what a felt object looks like. To do that, they built a data set of millions of tactile-visual pairings, gathered from webcam videos of 200 everyday objects being touched thousands of times by a GelSight tactical sensor. VisGel, the resulting data set, is being used to train AI models to come up with the likeliest matches of touch from visual data or images from tactical data.</p><p>HEARING THINGS</p><p>Sound plays an increasing role in helping robots differentiate between objects, which could help them detect hidden defects in products on an assembly line or determine the contents of unopened packages. Researchers at Carnegie Mellon created a database of digitized sounds and images by jostling around toy blocks, hand tools, apples, shoes and tennis balls, using a custom-made Tilt-Bot with a built-in tray attached to a robotic arm. They ultimately recorded more than 15,000 different sounds produced by 60 household objects. After the digitized data was fed into a machine-learning model, the trained system became so familiar with each sound it was able to correctly identify unseen objects about 75% of the time, the researchers say. Even more surprising, says Carnegie Mellon researcher Abhinav Gupta, was its ability now and then to take what it learned from listening to one set of objects and guess the general properties of similar objects.</p><p>A VOICE IN THE CROWD</p><p>Neural networks are also being trained to distinguish between voices and noise, or to separate multiple voices to focus on a main speaker -- both useful abilities in crowded rooms or busy streets. Douglas Beck, vice president of academic sciences at hearing aid maker Oticon Inc., says the approach uses a kind of digital-age filter. Algorithms are fed millions of speech samples, with and without background noise, to isolate the unique characteristics of human speech and suppress everything else. Once trained, whenever a device picks up and digitizes a sound, neural nets churn through data patterns to separate voices from a nearby jackhammer or zero in on the specific pattern of one voice. Neural net-enabled cochlear implants, guided by electric impulses in the brain, could someday enable people with profound hearing loss to regain control of what they hear -- or don't want to hear, Mr. Beck says.</p><p>PASSING THE SNIFF TEST</p><p>Aryballe, an AI software startup based in France, combines biosensors and machine learning to imitate the process our brains use to identify and differentiate between odors, an area known as digital olfaction. While the sensor picks up odor molecules in the air and encodes them into data representing unique digital signatures, neural networks mix and match that data within a massive database of previously analyzed traits, such as rancid, sweet, fruity or vinegary, or unique combinations. It can also be trained to pick out specific fragrances, like a type of cocoa bean or brand of perfume. \"Chanel No. 5 is supposed to smell a certain way. While a fake at first smells similar, we can be absolutely sure it's not Chanel,\" says Aryballe Chief Executive Sam Guilame.</p><p>STOP AND SMELL THE ROSES</p><p>A pair of researchers at the University of California, Riverside, is using a machine-learning system that can predict how people would react to a fragrance before they smell it. They examined patterns of odorant-receptor activity in the nasal cavity when it was exposed to different compounds, in categories like \"lemony\" and \"wet dog.\" They then created a giant database of these patterns observed in roughly 40 receptors -- humans have some 400 -- and trained a software model to parse that data and gauge how receptors would react to different odors. \"Our modeling led us to a potential biological explanation for how we're able to pick out the smell of a rose among a bouquet of flowers,\" says Anandasankar Ray, a professor of molecular, cell and systems biology who led the research. The system can be used to discover natural replacements for toxic or harsh chemicals in cosmetics and food, without consumers noticing a difference, he says.</p>",
  "published": "2021-07-09T00:00:00.000Z",
  "tags": [
    {
      "id": "US0378331005",
      "name": "Apple Inc.",
      "offsets": [
        {
          "start": 2870,
          "end": 2875
        }
      ],
      "nexusId": "10022657"
    }
  ]
}