{
  "_id": "51fc23b79a0a4b03261dbf8ada3c613c42ef4691aa138a44e551a76864495580",
  "feed": "wall-street-journal",
  "title": "The Future of Everything: The Health Issue --- Brain-Computer Connections: Neurosurgeon Eddie Chang on advances he envisions for BCI implants",
  "text": "<p>The amount of research in brain-computer interfaces, or BCI, has been growing steadily, according to federal online database Pubmed. In 2021, there were almost 600 studies, compared with roughly 340 in 2016. Fueling some of this activity are the availability of more powerful computers, improvements in artificial intelligence, and the further miniaturization of devices that can be implanted into the brain.</p><p>Dr. Chang says the aim is to decipher the brain activity that underpins complex human behaviors, like speech and emotions, in an attempt to develop therapies that could help people who can't speak or suffer from neuropsychiatric conditions, like depression and anxiety.</p><p>That promise also comes with some peril, such as the potential of further erosion of privacy as BCIs give direct access to the brain and the processes that underlie thoughts.</p><p>Dr. Chang spoke with The Wall Street Journal about where the field is going and his work on restoring speech and improving mood.</p><p>WSJ: Why speech?</p><p>A: Speech is really special. [It's] just one of the unique and defining behaviors we have as a species. And so it's been quite exciting to be trying to understand how the human brain processes such a unique behavior.</p><p>WSJ: What makes that difficult?</p><p>A: One of the biggest challenges is trying to translate electrical signals. The brain uses [electrical signals] as its own language for communication. That language has its own logic, its own code. And the true challenge is to understand how that code works.</p><p>WSJ: Has technology made that challenge easier?</p><p>A: The signals that we are interpreting -- they look nothing like what we're trying to decode, like words. They look like squiggles on a screen. And the patterns are so complex. A lot of our work now leverages computer-science advances in artificial intelligence and machine learning because they are very, very powerful ways of pattern recognition. So the kind of stuff that you use for Siri or Alexa.</p><p>WSJ: How do you see the technology evolving?</p><p>A: Ten to 15 years ago, the state of the field was at the level of trying to decode vowels, for example. What has been really incredible over the last five years is how much progress has been made from moving beyond just individual sounds to trying to decode words. And it's still really at its very beginning.</p><p>The first project that we published on this last year really focused on a vocabulary of 50 words. Our current efforts are towards two endeavors. One is to expand the vocabulary beyond our [Weill Institute's] 50-word vocabulary while also having high accuracy.</p><p>The second thing that we're trying to do is move from essentially trying to say a word and having a word appear on the screen to synthesizing words that you can hear. So the goal is for someone who's paralyzed not to just think of a word and have it appear on a screen for writing things out, but for actually synthesizing those sounds. And that turns out to be a very, very difficult and big challenge because of the complexity of producing words audibly [for] a very large vocabulary. For a small number of words, it's very doable now.</p><p>WSJ: How is that different from turning text on a screen to words you can hear?</p><p>A: Speech synthesis to us is not just the words themselves, but the nuance of creating the full richness of voice, like intonation and rhythm, in real time. For example, \"Sally went to the store.\" To change that from a statement to a question, all I've done there is increase the pitch of my voice on the last word. It's the same words, [but] what that does is that changes the meaning. So we're trying to tap into that as well.</p><p>WSJ: How are you using BCI to help patients with mental-health conditions?</p><p>A: We're really interested in trying to understand what is going on when someone is processing emotions normally, and what the signals look like in people who have depression and don't have normal regulation of their mood. Our hope is that by understanding these electrical signaling patterns that we can [use them] as biomarkers, as ways to understand when and what parts of the brain are involved when someone is having depressive episodes.</p><p>And then the second thing, which is far more important, is to use that information to intervene and to regulate some of these areas so that someone feels more normal, like they aren't in incapacitating depression.</p><p>WSJ: Psychedelics are another class of exciting experimental therapies in mental health. Do you think there will be crossover between that field and yours?</p><p>A: I do have a feeling that those worlds will combine. They're very complementary. [Psychedelics are] primarily a chemical approach. We're exploring this complementary signal, which is the electrical signal. The brain uses both heavily and they are related directly.</p><p>WSJ: Would you get a BCI for yourself, not as treatment, but for augmentation?</p><p>A: I personally wouldn't. There are so many things that I wish I were better at. But I personally really respect who I am as I am right now.</p><p>WSJ: Is that because of privacy? Why the hesitation?</p><p>A: Part of the hesitation is that there are risks. We're talking brain surgery. The second thing is that we're still very much in the learning phase. We're really far from any kind of scenario where I can imagine it being for augmentation. There's also an ethical line. I'm not supportive right now until we have a much, much better regulatory ethical framework around this.</p><p>---</p><p>This interview has been condensed and edited.</p>",
  "published": "2022-09-08T00:00:00.000Z",
  "tags": [
    {
      "id": "US0378331005",
      "nexusId": "10022657",
      "name": "Apple Inc.",
      "offsets": [],
      "weightsV2": {
        "weight": 1.0,
        "baseWeight": 0.0,
        "expansionWeight": 1.0,
        "ontologyWeight": 0.0,
        "tagPositionWeight": 0.0,
        "titleWeight": 0.0
      },
      "typeRank": 2
    }
  ]
}