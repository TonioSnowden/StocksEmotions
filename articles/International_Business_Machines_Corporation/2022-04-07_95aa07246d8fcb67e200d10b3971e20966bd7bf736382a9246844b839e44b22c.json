{
  "_id": "95aa07246d8fcb67e200d10b3971e20966bd7bf736382a9246844b839e44b22c",
  "feed": "wall-street-journal",
  "title": "How AI and Humans Can Best Collaborate at Work; Who decides who does what? And how can humans learn to trust AI? Research offers some answers.",
  "text": "<p>Research shows that humans and machines working together often perform better than either humans or artificial intelligence alone. Anecdote suggests the same: Chess grandmaster Garry Kasparov responded to his 1997 defeat by the IBM supercomputer Deep Blue by introducing a new form of chess in which humans could consult with computers before deciding on moves. The idea that human-machine teams could outperform in chess if they know how to work together then began to gain steam.</p><p>But even if you're sold on the idea that the future of work is human-machine partnership, how exactly does one collaborate productively with AI? It isn't as if a human and AI can meet over lunch and hash it out.</p><p>Still, as with cooperation among people, the most effective AI-human combinations often revolve around two key points. First, who decides who does what? And second, establishing—and maintaining—trust. It is these tasks that get too little attention in the moral panic over whether AI will put humans out of work.</p><p>Who delegates?</p><p>Determining how responsibilities should be delegated—or more specifically, who should do the delegating—is a critical component of effective human-machine partnerships.</p><p>Here, research offers some clues. In a recent study, researchers at the University of Cologne and the University of Minnesota asked people and machines—in some cases working together and at other times working alone—to match a set of images to one of 10 categories. (Image classification is a commonly used task in AI-human studies since the skills required can be generalized to a range of settings, from radiology to autonomous driving.) In one of the groups, the machine could delegate to a human if it was having trouble making a decision—say, whether a tricycle built for two should be classified with tandem bikes or rickshaws. In another group, humans could delegate to the computer if they were having trouble completing the task.</p><p>SHARE YOUR THOUGHTS</p><p>What would make you most likely to trust AI to help you accomplish your work responsibilities? Join the conversation below.</p><p>The results suggest humans aren't good at deciding who is best for the job: The teams in which the humans had the power to dole out work to AI made more categorization mistakes than the teams in which AI delegated tasks to humans, and more than AI working alone. (Humans working alone turned in the worst performance.) People, it seems, are poor judges of their own limitations. The humans in the study failed to ask AI for help when they needed it, and thus lost out on the chance to benefit from AI's recommendations.</p><p>The results suggest that in many cases—especially if a task is repetitive and one for which we have large training data sets (for example, routine X-ray scans)—it might be better for the AI system to be in control and delegate to humans when the system is in doubt.</p><p>Consider an auto insurance company seeking to automate part of its claims processing. Such a system might allow consumers to take photos of auto damage and submit their claims using a mobile app. AI that has been trained on photos and other information from past claims could then assess the legitimacy of the claims and the extent of damage. The machine predictions would be accompanied by a score estimating the degree to which AI was confident in its estimate. A low score would trigger AI to delegate the claim to human agents, who can intuit and spend more time on complex cases. Over time, it is possible that AI could handle a majority of the claims.</p><p>Building trust</p><p>Delegation isn't the only hurdle when nurturing AI-people partnerships. There's also humans' aversion to algorithms.</p><p>Research finds that people often don't trust or adopt AI's recommendations, even for tasks where algorithms have been shown to be superior. Even in areas where people don't consider themselves to be experts, they tend to resist taking AI's advice.</p><p>Over the years, researchers (myself included) have theorized that the reason people don't trust AI's advice is because they don't understand the decision-making process behind the algorithm.</p><p>But in recent research looking at how well people integrated AI advice when making predictions about who might suit whom in an online speed-dating event, my collaborators and I found that people's trust in machines didn't improve all that much when the machine predictions came with detailed explanations. What helped more was when the study subjects could observe the AI's record over time. That is, the longer they worked with AI and could observe its performance, the more they trusted it.</p><p>But even here, it is more complicated than just giving AI-human teams time to get to know each other. Research shows that people react more harshly to AI errors than human errors: After seeing both an algorithm and a human make the same mistake, people's confidence in the algorithm decreases more quickly than their confidence in the human. We also observed that trust in AI dropped when humans observed AI making an occasional error that a human wouldn't—even if AI on average was better overall at performing a specific task.</p><p>One solution might be to present AI less as an infallible superintelligence and more as a co-worker of sorts, one that is superior in some areas, eager to improve in others and flawed in ways that humans aren't. Humans and AI arrive at predictions through very different approaches, which is why they make different errors. These differences, though, are exactly why humans and AI complement each other so well.</p><p>In the short term, organizations might have to give up some degree of efficiency to promote human trust in machines. Research published in 2016 shows that people's trust in algorithms shoots up when they have some control over them. Being allowed to make even small tweaks to an algorithm's forecast or to reject the forecast a fraction of times increases the chances that a person will trust and use AI, improving performance overall, the researchers found. A bonus beyond the increase in trust is that a human in the loop might be more likely to detect the rudimentary errors that even high-performing AI can sometimes make.</p><p>Consider a large residential construction company that sells tens of thousands of new homes every year and wants to adopt an AI system to help it set prices for new homes. Such a system might be able to better incorporate consumer sentiment, local market trends and pricing data across markets to generate a price than humans. But giving managers in local markets the freedom to take AI forecasts and tweak them a bit is more likely to drive trust and adoption.</p><p>Similarly, an AI system trained to make loan-approval decisions based on millions of past cases is likely to be more accurate than most loan officers. But giving loan officers the ability to exercise judgment and overrule the AI system's loan recommendation some of the time might make them more willing to adopt the system.</p><p>Ultimately, we need to get better at understanding our strengths and limits relative to AI. It is perhaps not unlike how human chess players have gotten better by consulting AI. If this works for a chess grandmaster, it can work for organizations, too.</p><p>Dr. Hosanagar (@khosanagar) is a professor studying technology and digital business at the Wharton School of the University of Pennsylvania and faculty co-lead of AI for Business. He also is the author of \"A Human's Guide to Machine Intelligence.\" Email him at reports@wsj.com.</p><p>How AI and Humans Can Best Collaborate at Work</p>",
  "published": "2022-04-07T15:05:00.000Z",
  "tags": [
    {
      "id": "US4592001014",
      "nexusId": "10017781",
      "name": "International Business Machines Corporation",
      "offsets": [
        {
          "start": 228,
          "end": 231
        }
      ]
    }
  ]
}