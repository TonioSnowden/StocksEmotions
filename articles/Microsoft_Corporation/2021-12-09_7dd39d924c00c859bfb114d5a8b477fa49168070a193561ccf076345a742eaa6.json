{
  "_id": "7dd39d924c00c859bfb114d5a8b477fa49168070a193561ccf076345a742eaa6",
  "feed": "wall-street-journal",
  "title": "The Future of Everything: The Data Issue --- The Climate Model Deluge: Scientists are scrambling to store and analyze a surge of meticulous measurements of the planet. Their efforts will inform policy across the globe.",
  "text": "<p>For decades, scientists working to predict changes in the climate relied mostly on calculations involving simple laws of physics and chemistry but little data from the real world. But with temperatures world-wide continuing to rise -- and with data-collection techniques and technologies continuing to advance -- scientists now rely on meticulous measurements of temperatures, ocean currents, soil moisture, air quality, cloud cover and hundreds of other phenomena on Earth and in its atmosphere.</p><p>Reliable, readily available data is of critical importance to governments working to set policy and monitor compliance with international climate pacts, as well as to local authorities trying to help their communities adapt to unusual weather patterns or rising seas.</p><p>\"Now we can truly do climate studies because now we have observations to precisely say how weather trends have changed and are changing,\" says Suresh Vannan, who manages the National Aeronautics and Space Administration's physical oceanography archive center at the Jet Propulsion Laboratory in Southern California -- one of a dozen earth sciences data centers maintained by the space agency.</p><p>\"When you are trying to develop long-term environmental records, including climate records, consistent measurement is incredibly valuable,\" says Kevin Murphy, who as NASA's chief science data officer oversees an archive of Earth observation data used by 3.9 million people last year. \"It's irreplaceable data.\"</p><p>And with ever-rising numbers of sensor-studded satellites, aircraft, ocean buoys and the like, there's more data all the time. Over the next decade, officials managing the main U.S. repositories of climate-related information expect their archives' total volume to grow from about 83 petabytes today to more than 650 petabytes. One petabyte of digital memory can hold thousands of feature-length movies, with 650 enough to hold the contents of the Library of Congress 30 times over.</p><p>All that information, though, is more than conventional data storage can handle and more than any human mind can readily assimilate, data scientists say. To accommodate it all, the federal workers tasked with managing the data are moving it into the cloud, which offers almost unlimited memory storage while eliminating the need for scientists to maintain their own on-site archives. In addition, these archive managers are devising new analytical techniques and adapting a standard format for the data no matter who collected it and who wants to study it.</p><p>They are reinventing climate science from the ground up.</p><p>\"We are in the midst of a technology evolution,\" says Nancy Ritchie, archive branch chief at the National Centers for Environmental Information in Asheville, N.C., which maintains a trove of data about the weather, land, atmosphere and oceans for the National Atmospheric and Oceanic Administration (NOAA).</p><p>Satellites are key to the story. As of last September, government agencies and private companies had about 900 Earth-orbiting satellites gathering data about our planet, according to the Union of Concerned Scientists. That is almost three times as many as were aloft in 2008. More are being readied for launch.</p><p>\"This is a new era for Earth observation missions, and the huge amount of data they will generate requires a new era of data handling,\" Dr. Murphy says. \"We are about to launch a series of really high data rate missions.\"</p><p>NASA's $1 billion Surface Water and Ocean Topography mission will measure Earth's lakes, rivers and oceans in the first detailed global survey of the planet's surface water. The SUV-size satellite, which is slated to launch in November, is expected to transmit about a terabyte of data every day.</p><p>That is a drop in the data bucket compared with the space agency's $1.5 billion Nisar radar imaging satellite, which is scheduled for launch in January 2023. Its sensors will detect movements of the planet's land, ice sheets and sea ice as small as 0.4 inches, transmitting 80 terabytes of data every day.</p><p>With current data handling systems and typical internet connections, it would take about a year to download just four days' worth of Nisar data, says Dr. Murphy.</p><p>To speed access to the data and lower the cost of computer equipment, NASA and NOAA are working with Amazon Web Services, Google Cloud and Microsoft Corp. to move their climate databases into the cloud. NOAA expects to upload its entire Earth science archive into the cloud by 2027, Ms. Ritchie says.</p><p>For some scientists, the influx of so much information offers an opportunity to weed out uncertainties in the 100 or so computer climate models used to project climate change. Earlier this year, the United Nations Intergovernmental Panel on Climate Change for the first time used data on past climate behavior to gauge the reliability of climate models for policy makers.</p><p>Data managers are also reorganizing digital climate information to make it easier for machine learning and artificial intelligence systems to access it autonomously. Scientists say AI tools may be able to detect subtle patterns in the data, such as hurricane intensity or ocean eddies, that elude detection by humans.</p><p>\"Humans can only view and understand things in a very constructed and organized fashion,\" Ms. Ritchie says. \"We like easily readable file names or directory structures because that is the way our brain likes to organize things. When we move to bots, they don't care about directory structures.\"</p><p>Columbia University recently opened a $25 million center to harness AI for climate modeling. And the Climate Modeling Alliance, a consortium of 70 climate scientists, engineers and mathematicians, is working on an experimental AI climate model with $25 million mainly from the National Science Foundation and Eric Schmidt, the former chief executive of Google, and his wife, Wendy Schmidt.</p><p>\"Situations of overwhelming complexity and absurd amounts of data are a sweet spot for AI,\" says Michael Pritchard, principal investigator at the computational clouds and climate laboratory at the University of California, Irvine. \"We are drowning in data and it is not always clear how to use it.\"</p>",
  "published": "2021-12-09T00:00:00.000Z",
  "tags": [
    {
      "id": "US5949181045",
      "name": "Microsoft Corporation",
      "offsets": [
        {
          "start": 4310,
          "end": 4319
        },
        {
          "start": 4310,
          "end": 4325
        }
      ],
      "nexusId": "10031144"
    }
  ]
}