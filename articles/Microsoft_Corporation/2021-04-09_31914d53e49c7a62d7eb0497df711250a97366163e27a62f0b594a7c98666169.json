{
  "_id": "31914d53e49c7a62d7eb0497df711250a97366163e27a62f0b594a7c98666169",
  "feed": "wall-street-journal",
  "title": "Technology:  Facebook Releases Data Set  To Assess Bias in Algorithms  ----  By John McCormick",
  "text": "<p>   Facebook Inc. made publicly available a data set designed to help artificial-intelligence researchers evaluate their computer vision and audio models for potential algorithmic bias. </p><p>   The data set, called Casual Conversations, consists of videos of some 3,000 participants of various skin tones sharing their age and gender. </p><p>   Cristian Canton Ferrer, Facebook AI's research manager who supervised the effort, said the data set tries to address two problems: \"The critical need within the AI community to [improve] the fairness of AI systems\" and \"the lack of high-quality data sets that are designed to help measure this fairness in AI.\" Facebook AI is the social network's artificial-intelligence organization. </p><p>   Artificial-intelligence systems are trained on large sets of data. Facial-recognition systems, for instance, are fed mountains of facial images that allow the system to find patterns in faces that it can use to make a match. If a data set used to train a system included mostly the photos of a particular demographic, it might not be as accurate when identifying photos of people in other demographics. AI systems have been shown to be less accurate at identifying faces of dark-skinned women, for example. </p><p>   Facebook said having people provide their ages and genders for content labeling, as opposed to having a third party or computer system estimate that information, creates a relatively unbiased data set of people's actual ages and genders. </p><p>   The Casual Conversations data set also includes labels of participants' apparent skin tones that were developed by trained annotators using the Fitzpatrick scale, a skin classification system. The annotators also marked videos with ambient lighting conditions, which can help measure how AI systems treat skin tones under lowlight conditions. </p><p>   In all, participants made an average of 15 videos each, in which they engaged in nonscripted conversations, for a total of more than 45,000 videos. The videos were originally gathered as part of an earlier project Facebook participated in called the Deepfake Detection Challenge, which was set up to accelerate research for detecting and preventing manipulated media. </p><p>   Many companies have released tools designed to check algorithms for bias in recent years. LinkedIn Fairness Toolkit, introduced last year by Microsoft Corp.'s professional social-network unit, analyzes the attributes of a data set, such as its racial and gender makeup, and compares its findings with an algorithm's results. If a data set is nearly equally divided by gender, for example, but a search algorithm based on that data set generates results with only a quarter of women, the system will spot that. </p><p>   The standard way of evaluating AI models' performance today is to measure them against a test set after the models have been trained and validated, Facebook said. But that test set, the company said, may contain the same shortcomings as the training sets because it may be collected from similar sources. </p><p>   Svetlana Sicular, a vice president, analyst, at technology research and advisory firm Gartner Inc., said such a second set of eyes can help AI developers validate the fairness of their systems. </p><p></p>",
  "published": "2021-04-09T06:06:00.000Z",
  "tags": [
    {
      "id": "US5949181045",
      "name": "Microsoft Corporation",
      "offsets": [
        {
          "start": 2305,
          "end": 2320
        },
        {
          "start": 2305,
          "end": 2314
        }
      ],
      "nexusId": "10031144"
    }
  ]
}