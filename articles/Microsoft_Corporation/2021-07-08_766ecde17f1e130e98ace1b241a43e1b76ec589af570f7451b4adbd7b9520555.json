{
  "_id": "766ecde17f1e130e98ace1b241a43e1b76ec589af570f7451b4adbd7b9520555",
  "feed": "wall-street-journal",
  "title": "YouTube Algorithm Found to Still Push Harmful Content",
  "text": "<p>The study highlights the continuing challenge Alphabet Inc. subsidiary YouTube faces as it tries to police the user-generated content that turned it into the world's leading video service. It is emblematic of the struggle roiling platforms from Facebook Inc. to Twitter Inc., which soared to prominence by encouraging people to share information but which now face regulatory and social pressure to police divisive, misleading and dangerous content without censoring diverse points of view.</p><p>For YouTube, it also shows gaps in its efforts to steer users to videos that should be of interest based on viewer patterns, as opposed to those that are going viral for other reasons.</p><p>In the study, one of the largest of its kind, 37,000 volunteers used a browser extension that tracked their YouTube usage over a 10-month period ended in May. When a participant flagged a video as problematic, the extension was able to track whether it was recommended to the viewer or whether the person found it independently. The videos flagged by participants as objectionable included a sexualized parody of \"Toy Story\" and an election video falsely suggesting Microsoft Corp. founder Bill Gates hired students involved with Black Lives Matter to count ballots in battleground states.</p><p>YouTube later removed 200 videos that participants flagged, which accumulated more than 160 million views before they were taken down, according to Mozilla.</p><p>A YouTube spokesman said the company has reduced recommendations of content it defines as harmful to below 1% of videos viewed. He said it launched 30 changes over the past year to address the issue. The company's safety team says its automated system now detects 94% of videos that violate YouTube policies and removes most of them before they receive 10 views.</p><p>\"The goal of our recommendation system is to connect viewers with content they love,\" the spokesman said.</p><p>YouTube has been dogged for years by concerns about the dangers of videos on its site, including for children. Last year, it pulled down channels promoting QAnon that it said could encourage harm.</p><p>The company credits its recommendation algorithm with driving more than two-thirds of the 1 billion hours in daily viewership that helped generate $19.7 billion in revenue last year. Those videos -- 720,000 hours of video gets uploaded daily -- aren't screened by human moderators before going live.</p><p>The result is a Whac-A-Mole system where YouTube uses machine learning to try to catch videos that violate its policies before viewers see them. Since 2019, the company says, its policing algorithms have reduced so-called borderline content by 70%.</p><p>Mozilla Senior Manager Brandi Geurkink, who led the study, said the research accentuated what she called an inherent contradiction between YouTube's algorithms, with one set recommending problematic videos while another tries to remove them. She noted that participants seldom reported finding problematic videos when they searched for content on their own, but its algorithm recommended things that they didn't want to see.</p><p>\"If the users say, 'These recommendations aren't working for us,' then who is the recommendation algorithm servicing?\" said Ms. Geurkink. The company says its surveys show users are generally satisfied with what it recommends.</p><p>Mozilla owns and operates the Firefox internet browser, which competes with Alphabet's Chrome browser.</p>",
  "published": "2021-07-08T00:00:00.000Z",
  "tags": [
    {
      "id": "US5949181045",
      "name": "Microsoft Corporation",
      "offsets": [
        {
          "start": 1142,
          "end": 1157
        },
        {
          "start": 1142,
          "end": 1151
        }
      ],
      "nexusId": "10031144"
    }
  ]
}