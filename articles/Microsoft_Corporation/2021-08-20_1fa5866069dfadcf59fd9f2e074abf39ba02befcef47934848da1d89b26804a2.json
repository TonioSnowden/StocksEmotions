{
  "_id": "1fa5866069dfadcf59fd9f2e074abf39ba02befcef47934848da1d89b26804a2",
  "feed": "ftcomall",
  "title": "Inside the mind of Big AI",
  "text": "<p>Is the day approaching when computers start to learn tricks they weren't designed to handle? Or develop deceptive behaviours that are hard to see through? Or come to truly “understand” the information they're working on, raising <a href=\"https://www.ft.com/content/512cef1d-233b-4dd8-96a4-0af07bb9ff60\">philosophical questions</a> about the boundaries between human and machine?</p> <p>Serious AI researchers have long argued that questions such as these raise unreal expectations about their field and should stay in the realm of science fiction. Today's AI systems, we are told, are boring number-crunchers, churning through massive data sets to draw their inferences.</p> <p>So what happens when the researchers themselves suggest that these sci-fi storylines are no longer as far fetched as they once sounded?</p> <p>Something significant has crept up on the AI world. Building ever-larger AI models has been all the craze for the past two years, as researchers have corralled huge computing resources and giant data sets to create evermore powerful systems.</p> <p>Sheer scale, it turns out, may have opened the door to something new: an era of more general-purpose AI systems that can turn their hand to many different tasks — and whose full capabilities can no longer be predicted with absolute certainty.</p> <p>The best known of these is OpenAI's text-generating system, <a href=\"https://www.ft.com/content/beaae8b3-d8ac-417c-b364-383e8acd6c8b\">GPT-3</a>. Its ability to sustain an extended monologue on any given subject brought a rare burst of popular attention to AI research, even if the system did often wander off into incoherence. But companies such as Google, Microsoft and Huawei have been hard at work on similar systems. Even some start-ups have joined the race: Israeli's AI21 Labs showed off a system last week that is bigger than GPT-3.</p> <p>Exactly where this will lead is hard to tell. The scenarios at the start of this article — such as computers developing unexpected skills, or a deeper level of understanding — are among the possibilities that more than 100 researchers from the Stanford Institute for Human-Centered AI <a href=\"https://arxiv.org/abs/2108.07258\">raised this week</a> in an attempt to draw attention to where they think the era of giant AI models is taking us.</p> <p>The researchers generally say it is too early to tell if the new systems will develop these capabilities. But there aren't, in the words of some of them, “definitive reasons to think they could not”. When you can no longer be certain what your machines aren't capable of, it may be time to start worrying.</p> <p>Size has had two important results, to judge by the large language models that are at the forefront of this new trend.</p> <p>One is that the systems have started to demonstrate a more generalisable intelligence that can be applied to a number of different tasks. This is a big deal in AI. Until now, machine learning systems have been highly inflexible, and it has been hard to transfer a skill learned on one problem to another. With huge size and the use of a new learning technique called transformers, this limitation seems to be falling away.</p> <p>Their scale and adaptiveness could turn systems such as this into a new base layer of intelligence — what the Stanford researchers call foundation models. Developers working on specific applications of AI in, say, law or healthcare wouldn't need to reinvent the wheel: they could call on a language system to provide the more generic capabilities.&#xa0;</p> <p>This idea need not be limited to language systems. <a href=\"https://www.ft.com/content/a78997a7-ad25-40b9-b723-55db143e26db\">Google's MUM</a> and similar “multitask” systems are already applying it to images. They could be followed, the researchers suggest, by a general-purpose reasoning capability, a flexible robotics model, and an AI that has mastered interaction with humans.</p> <p>The prospect of pervasive new base-layer intelligences like this raises some obvious questions. Who should build and control them? And, given that they might become common building blocks for many of the world's more specialised AI systems, what happens when something goes wrong?</p> <p>The second interesting feature of very large AI models, meanwhile, has been their apparent ability to spontaneously learn new tricks. This is where things get weird. GPT-3 came up with a technique known as in-context learning to master new problems — even though its developers didn't teach it to do this and a smaller predecessor, built to the same design, didn't think of the idea.</p> <p>So-called emergent capabilities such as this are the big unknown of large AI models. What happens when the people who build a system can no longer anticipate the full powers it will develop? The results might be hugely beneficial if the computers come up with new ways to solve intractable problems that their makers haven't thought of. But there is also an obvious downside to machines working things out for themselves.</p> <p>For now, exactly where this may lead is still confined to the pages of science fiction. But with the race to build ever-larger AI models gathering pace, we may not have to wait long to find out.</p> <p><a href=\"mailto:richard.waters@ft.com\"><em>richard.waters@ft.com</em></a></p><p>Source: Richard Waters 2021 'Inside the mind of Big AI' FT.com 20 August. Used under licence from the Financial Times. © The Financial Times Limited 2021. All Rights Reserved. </p><p>Please do not cut and paste FT articles and redistribute by email or post to the web.</p>",
  "published": "2021-08-20T04:00:35.379Z",
  "tags": [
    {
      "id": "US5949181045",
      "name": "Microsoft Corporation",
      "offsets": [
        {
          "start": 1486,
          "end": 1495
        }
      ],
      "nexusId": "10031144"
    }
  ]
}