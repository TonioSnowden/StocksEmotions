{
  "_id": "813ac93c27918c2dbed03e0395a7acd3d651a8c16df853a2f123663d8195d9be",
  "feed": "wall-street-journal",
  "title": "AI Ethics Teams Bulk Up in Size, Influence at Tech Firms; A flaw in a photo-cropping algorithm at Twitter prompted the attention of its ethics team",
  "text": "<p>The algorithm worked by estimating what a person might want to see first in a picture so that the system could determine how to crop the image to an easily viewable size. Late last year, Twitter started receiving feedback from its users that the feature was biased toward white people.</p><p>An investigation by the META team earlier this month found that the algorithm deviated from demographic parity by 4 percentage points—meaning that when all things in the photo were equal, the cropping function would prefer a white individual over a Black individual. The META team's finding supported the company's decision to stop using the algorithm, according to Ms. Chowdhury.</p><p>The conclusion: Not every feature on Twitter is a good candidate for an algorithm and how to crop an image is a decision best made by people, Ms. Chowdhury said.</p><p>The episode demonstrates how company AI ethics teams, often a mixture of data scientists, machine learning engineers and subject matter experts, are set to play an even larger role as AI software use goes mainstream and the potential penalties for biased algorithms go beyond a temporary setback in company reputation.</p><p>\"A lot of companies have burned through their trust capital,\" said Ms. Chowdhury.</p><p>Demand for AI is surging. Some 32% of global organizations now have AI initiatives in production, up from 11% in 2019, according to tech industry research firm International Data Corp .</p><p>Paralleling AI's rise in use have been a series of misfires, including facial recognition systems revealed to be less accurate at identifying the faces of people with dark skin  and credit-card applications that gave women lower limits than their husbands.</p><p>These episodes, as well as legislative moves to regulate the technology , have prompted tech companies to build out their AI ethics teams.</p><p>Alphabet Inc .'s Google and Twitter both say they are doubling the size of their AI ethics teams . Accenture PLC and Salesforce.com Inc . also said they plan to accelerate hiring. DataRobot Inc ., an enterprise AI platform company, is looking to expand its AI ethics team this year to 36 people from 30.</p><p>As teams grow so does the role they play in their companies' AI efforts, from running software risk assessments and developing fairness and explainability tools, to actually working with product development teams to mitigate issues in software releases.</p><p>\"As [AI] moves out of the lab and into scale deployment, that's where organizations are really aware of the need to have these kinds of controls,\" said Ray Eitel-Porter, Accenture 's global lead for Responsible AI.</p><p>Accenture , as part of its AI development process, enlists its ethics team to act as devil's advocates, questioning how a model is being built to expose possible flaws. The goal is to find any and all potential problems early, when they are the easiest to fix. Accenture is expanding its ethics team but didn't provide specifics.</p><p>When Salesforce.com was developing a sentiment analysis model a couple of years ago for its Einstein AI technology, an engineer noticed that if a user entered a sentence that included one of about a dozen different identity markers, such as Black, Muslim, or feminist, the analysis model would automatically mark that sentiment as negative regardless of the rest of the sentence, according to Kathy Baxter, principal architect of ethical AI at the company.</p><p>The product team made adjustments to address the problem and was getting ready to move the model into production, but the ethics team—realizing the sensitive nature of the issue—recommended that the team conduct additional tests to ensure the system's performance, which it did before its release, Ms. Baxter said.</p><p>\"It wasn't a contentious moment. The team recognized ‘This isn't a good thing,'\" Ms. Baxter said.</p><p>International Business Machines Corp.'s IBM Research Trusted AI team, a dedicated group of 100 or so researchers, develop new techniques and tools to address fairness, explainability and other areas. The team built AI Fairness 360, a tool that application developers can use to audit models for bias, and AI Explainability 360, a forensic tool that is designed to help an organization figure out how an AI model makes a decision.</p><p>Svetlana Sicular, a vice president, analyst, at technology research and advisory firm Gartner Inc ., said those AI companies that can make the investment in hiring AI ethicists and building responsible AI systems that minimize risks related to privacy, bias and fairness, have a competitive edge in the marketplace—something the technology companies recognize.</p><p>\"I think customers are looking at [AI ethics] and realizing there's a desire to do good and not amplify systemic problems,\" said Ted Kwartler, vice president of AI trust at DataRobot .</p><p>Write to John McCormick at john.mccormick@wsj.com</p>",
  "published": "2021-05-27T11:00:00.000Z",
  "tags": [
    {
      "id": "US79466L3024",
      "name": "salesforce.com, inc.",
      "offsets": [
        {
          "start": 1928,
          "end": 1942
        },
        {
          "start": 2919,
          "end": 2933
        }
      ],
      "nexusId": "10009678"
    }
  ]
}